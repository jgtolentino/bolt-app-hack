# üöÄ Groq AI Integration

I've successfully added Groq as an AI provider option for your dashboard. Groq offers fast inference with open-source models like Mixtral and Llama.

## ‚úÖ What's Been Added

### 1. **Credentials Configuration**
- Added Groq API configuration in `src/config/credentials.ts`
- Default model: `mixtral-8x7b-32768` (Mixtral 8x7B with 32k context)
- Environment variable: `VITE_GROQ_API_KEY`

### 2. **AI Service Support**
- Updated `src/services/aiService.ts` to support Groq
- Added `callGroq()` method for API calls
- Groq can be used as primary or fallback provider
- Automatic fallback chain: OpenAI ‚Üí Anthropic ‚Üí Groq

### 3. **RAG Engine Support**
- Updated `src/services/rag-engine.ts` for SQL generation
- Groq is tried after OpenAI and Anthropic
- Uses Mixtral for natural language to SQL conversion

## üîß How to Use Groq

### 1. Get a Groq API Key
1. Visit [console.groq.com](https://console.groq.com)
2. Sign up for a free account
3. Generate an API key from the dashboard

### 2. Add to Environment
```bash
# Add to your .env file
VITE_GROQ_API_KEY=gsk_your_groq_api_key_here
```

### 3. Set Groq as Primary Provider (Optional)
```typescript
// In your code, specify Groq as the provider
const aiService = new AIService({ provider: 'groq' });
```

## üìä Supported Models

Groq supports several models that are automatically available:
- **Mixtral 8x7B** (default) - Great for complex reasoning
- **Llama 3 70B** - Powerful general-purpose model
- **Llama 3 8B** - Faster, lighter model
- **Gemma 7B** - Google's efficient model

To use a different model, update your `.env`:
```bash
VITE_GROQ_MODEL=llama3-70b-8192  # For Llama 3 70B
```

## üéØ Benefits of Groq

1. **Speed**: Groq's LPU (Language Processing Unit) provides extremely fast inference
2. **Cost**: Generous free tier for development
3. **Open Models**: Access to best open-source models
4. **Reliability**: Great as a fallback when other providers are down

## üîÑ Fallback Chain

The system now has a robust 3-tier fallback:
1. **Primary**: Your chosen provider (OpenAI/Anthropic/Groq)
2. **Secondary**: Next available provider
3. **Tertiary**: Final fallback provider
4. **Mock Data**: If all fail, uses intelligent mock insights

## üí° Usage Examples

### For AI Insights:
```typescript
// Groq will be used if it's the only configured provider
// or if specified explicitly
const insight = await aiService.generateInsight(
  'peakHourAnalysis',
  data,
  filters
);
```

### For SQL Generation:
```typescript
// RAG Engine will try Groq after OpenAI/Anthropic
const result = await ragEngine.queryToSQL(
  "Show me top selling products by region"
);
```

## üß™ Test Your Integration

1. Add your Groq API key to `.env`
2. Restart the dev server: `npm run dev`
3. Check the console for: "Groq API Key: gsk_...xyz"
4. AI features will now use Groq when available

## üìù Notes

- Groq uses OpenAI-compatible API format
- Supports streaming responses (not implemented yet)
- Rate limits are generous for free tier
- Best for real-time applications due to speed

Your app now has triple redundancy for AI features! üéâ